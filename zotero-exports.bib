@inproceedings{10.1007/978-3-540-30115-8_7,
  title = {Applying Support Vector Machines to Imbalanced Datasets},
  booktitle = {Machine Learning: {{ECML}} 2004},
  author = {Akbani, Rehan and Kwek, Stephen and Japkowicz, Nathalie},
  editor = {Boulicaut, Jean-François and Esposito, Floriana and Giannotti, Fosca and Pedreschi, Dino},
  date = {2004},
  pages = {39--50},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  abstract = {Support Vector Machines (SVM) have been extensively studied and have shown remarkable success in many applications. However the success of SVM is very limited when it is applied to the problem of learning from imbalanced datasets in which negative instances heavily outnumber the positive instances (e.g. in gene profiling and detecting credit card fraud). This paper discusses the factors behind this failure and explains why the common strategy of undersampling the training data may not be the best choice for SVM. We then propose an algorithm for overcoming these problems which is based on a variant of the SMOTE algorithm by Chawla et al, combined with Veropoulos et al's different error costs algorithm. We compare the performance of our algorithm against these two algorithms, along with undersampling and regular SVM and show that our algorithm outperforms all of them.},
  isbn = {978-3-540-30115-8}
}

@article{1522531,
  title = {Top-down Induction of Decision Trees Classifiers - a Survey},
  author = {Rokach, L. and Maimon, O.},
  date = {2005},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {35},
  number = {4},
  pages = {476--487},
  doi = {10.1109/TSMCC.2004.843247},
  keywords = {Classification,Classification tree analysis,Data mining,decision trees,Decision trees,Industrial training,Loans and mortgages,Machine learning,Machine learning algorithms,Pattern recognition,Predictive models,pruning methods,splitting criteria,Statistics}
}

@article{354051491,
  title = {A Reliable Network Intrusion Detection Approach Using Decision Tree with Enhanced Data Quality},
  author = {Guezzaz, Azidine and Benkirane, Said and Azrour, Mourade and Khurram, Shahzada},
  date = {2021-08},
  journaltitle = {Security and Communication Networks},
  volume = {2021},
  doi = {10.1155/2021/1230593}
}

@inproceedings{6524743,
  title = {{{SVM}} Kernel Functions for Classification},
  booktitle = {2013 International Conference on Advances in Technology and Engineering ({{ICATE}})},
  author = {Patle, Arti and Chouhan, Deepak Singh},
  date = {2013},
  pages = {1--9},
  doi = {10.1109/ICAdTE.2013.6524743},
  keywords = {Accuracy,Data mining,feature,Kernel,Mathematical model,Polynomials,radial basis function,support vector,Support vector machines,Training}
}

@article{binary-classification-for-fraud-detection,
  title = {Investigating the Effectiveness of One-Class and Binary Classification for Fraud Detection},
  author = {Leevy, Joffrey and Hancock, John and Khoshgoftaar, Taghi and Abdollah Zadeh, Azadeh},
  date = {2023-10},
  journaltitle = {Journal of Big Data},
  volume = {10},
  doi = {10.1186/s40537-023-00825-1}
}

@book{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016-08-13},
  pages = {794},
  doi = {10.1145/2939672.2939785},
  pagetotal = {785}
}

@article{cortesSupportvectorNetworks1995b,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995-09-01},
  journaltitle = {Machine Learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {1573-0565},
  doi = {10.1007/BF00994018},
  url = {https://doi.org/10.1007/BF00994018},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.}
}

@article{friedmanGreedyFunctionApproximation2000,
  title = {Greedy {{Function Approximation}}: {{A Gradient Boosting Machine}}},
  author = {Friedman, Jerome},
  date = {2000-11-28},
  journaltitle = {The Annals of Statistics},
  volume = {29},
  doi = {10.1214/aos/1013203451}
}

@article{gatesOriginTokamakDensity2012,
  title = {Origin of {{Tokamak Density Limit Scalings}}},
  author = {Gates, D. A. and Delgado-Aparicio, L.},
  date = {2012-04-20},
  journaltitle = {Phys. Rev. Lett.},
  volume = {108},
  number = {16},
  pages = {165004},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.108.165004},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.108.165004},
  urldate = {2025-08-14},
  langid = {english},
  file = {/home/ruben/snap/zotero-snap/common/Zotero/storage/IXAQ4S8S/Gates y Delgado-Aparicio - 2012 - Origin of Tokamak Density Limit Scalings.pdf}
}

@article{greenwaldDensityLimitsToroidal2002,
  title = {Density Limits in Toroidal Plasmas},
  author = {Greenwald, Martin},
  date = {2002-08-01},
  journaltitle = {Plasma Phys. Control. Fusion},
  volume = {44},
  number = {8},
  pages = {R27-R53},
  issn = {07413335},
  doi = {10.1088/0741-3335/44/8/201},
  url = {https://iopscience.iop.org/article/10.1088/0741-3335/44/8/201},
  urldate = {2025-08-14}
}

@article{haririExtendedIsolationForest2019,
  title = {Extended {{Isolation Forest}} with {{Randomly Oriented Hyperplanes}}},
  author = {Hariri, Sahand and Kind, Matias and Brunner, Robert},
  date = {2019-10-31},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {PP},
  pages = {1--1},
  doi = {10.1109/TKDE.2019.2947676},
  abstract = {We present an extension to the model-free anomaly detection algorithm, Isolation Forest. This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. We motivate the problem using heat maps for anomaly scores. These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. We explain this problem in detail and demonstrate the mechanism by which it occurs visually. We then propose two different approaches for improving the situation. First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. This approach results in remedying the artifact seen in the anomaly score heat maps. We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.}
}

@inproceedings{inproceedings,
  title = {Isolation Forest},
  author = {Liu, Fei Tony and Ting, Kai and Zhou, Zhi-Hua},
  date = {2009-01},
  pages = {413--422},
  doi = {10.1109/ICDM.2008.17}
}

@online{kekulawalaSupportVectorMachines2024,
  title = {Support {{Vector Machines}}: {{A}} Mathematical Guide — {{Part}} 3},
  shorttitle = {Support {{Vector Machines}}},
  author = {Kekulawala, Chamuditha},
  date = {2024-06-11T09:02:32},
  url = {https://medium.com/@ckekula/support-vector-machines-a-mathematical-guide-part-3-78fd3c8bf44c},
  urldate = {2025-07-12},
  abstract = {In part 2 we talked about adding similarity features using the Gaussian RBF function. Now let’s go into the RBF Kernel trick.},
  langid = {english},
  organization = {Medium},
  file = {/home/ruben/snap/zotero-snap/common/Zotero/storage/5GZVFU77/support-vector-machines-a-mathematical-guide-part-3-78fd3c8bf44c.html}
}

@online{OutlierClassifier,
  title = {{{outlierClassifier}}},
  url = {https://github.com/outlierClassifier},
  urldate = {2025-08-14},
  abstract = {outlierClassifier has 11 repositories available. Follow their code on GitHub.},
  langid = {english},
  organization = {GitHub},
  file = {/home/ruben/snap/zotero-snap/common/Zotero/storage/IEGB5THM/outlierClassifier.html}
}

@software{OutlierClassifierOutlier_orchestrator2025a,
  title = {{{outlierClassifier}}/Outlier\_orchestrator},
  date = {2025-08-14T11:18:49Z},
  origdate = {2025-04-26T15:18:40Z},
  url = {https://github.com/outlierClassifier/outlier_orchestrator},
  urldate = {2025-08-14},
  abstract = {Model orchestrator for anomaly detection},
  organization = {outlierClassifier}
}

@online{OutlierClassifierOutlier_orchestratorOrquestador,
  title = {{{outlierClassifier}}/Outlier\_orchestrator: {{Orquestador}} de Modelos Para Detección de Anomalías},
  url = {https://github.com/outlierClassifier/outlier_orchestrator},
  urldate = {2025-08-14},
  file = {/home/ruben/snap/zotero-snap/common/Zotero/storage/F3PW3EZ5/outlier_orchestrator.html}
}

@software{OutlierClassifierPy_xgboost2025,
  title = {{{outlierClassifier}}/Py\_xgboost},
  date = {2025-08-03T08:12:39Z},
  origdate = {2025-04-29T11:54:35Z},
  url = {https://github.com/outlierClassifier/py_xgboost},
  urldate = {2025-08-12},
  abstract = {py\_xgboost},
  organization = {outlierClassifier}
}

@software{OutlierClassifierSvm2025,
  title = {{{outlierClassifier}}/Svm},
  date = {2025-05-02T11:11:15Z},
  origdate = {2025-04-26T19:45:07Z},
  url = {https://github.com/outlierClassifier/svm},
  urldate = {2025-08-14},
  abstract = {SVM classifier},
  organization = {outlierClassifier}
}

@article{puiattiHighDensityLimit2009,
  title = {High Density Limit in Reversed Field Pinches},
  author = {Puiatti, M. E. and Scarin, P. and Spizzo, G. and Valisa, M. and Paccagnella, R. and Predebon, I. and Agostini, M. and Alfier, A. and Canton, A. and Cappello, S. and Carraro, L. and Gazza, E. and Innocente, P. and Lorenzini, R. and Marrelli, L. and Terranova, D.},
  date = {2009-01-01},
  journaltitle = {Physics of Plasmas},
  volume = {16},
  number = {1},
  pages = {012505},
  issn = {1070-664X, 1089-7674},
  doi = {10.1063/1.3063060},
  url = {https://pubs.aip.org/pop/article/16/1/012505/261748/High-density-limit-in-reversed-field-pinches},
  urldate = {2025-08-14},
  abstract = {The fusion triple product depends on density, which is therefore a key parameter for the future fusion reactor. In this paper the high density limit is studied in the reversed field experiment (RFX-mod) device in Padova, Italy. A rather complete experimental picture of the high density regimes is provided, showing a series of features, such as, plasma flow inversion in the edge, density accumulation, radiation condensation (poloidally symmetric and toroidal asymmetric) which resemble the MARFE phenomenon characteristic of tokamak discharges. However, in RFX-mod high density does not cause a disruption, as often observed in tokamaks, but a soft landing of the plasma current. According to a new 1D transport/radiative code applied to analyze the high density discharges, the current decay is due to an increased need for dynamo in these highly resistive, edge-cooled discharges. The relation between the radiative pattern of RFX-mod high density plasmas, the magnetic topology, and edge radial electric field is discussed.},
  langid = {english}
}

@book{scholkopfLearningKernelsSupport2001,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  date = {2001-12-07},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/4175.001.0001},
  url = {https://doi.org/10.7551/mitpress/4175.001.0001},
  urldate = {2025-08-14},
  abstract = {A comprehensive introduction to Support Vector Machines and related kernel methods.In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs—-kernels—for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  isbn = {978-0-262-25693-3}
}
